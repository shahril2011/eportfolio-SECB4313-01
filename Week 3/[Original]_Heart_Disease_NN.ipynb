{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1q_qKIV2t2u"
      },
      "source": [
        "#Import all library needed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,BatchNormalization, Dropout\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "#confusion matrix visualization\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix,classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link notebook with google drive and access data from your personal Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "# Set the data path for dataset and model location\n",
        "dataset_dir = \"/content/gdrive/My Drive/Notes for Simulation & Modeling Class/[Work] Deep Learning Hands-on/Dataset/Heart Disease/\"\n",
        "model_loc = \"/content/gdrive/My Drive/Notes for Simulation & Modeling Class/[Work] Deep Learning Hands-on/Dataset/\"\n",
        "\n",
        "print(os.listdir(dataset_dir))\n",
        "data = pd.read_csv(dataset_dir+'heart.csv')"
      ],
      "metadata": {
        "id": "VMcZaXO2M3I-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "88512ff7-5021-4ede-b628-88bfae02161d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-617309d43151>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Link notebook with google drive and access data from your personal Gdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Set the data path for dataset and model location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZADep6q2t3D"
      },
      "source": [
        "# Exploratory data analysis (EDA) steps to analyze and investigate datasets.\n",
        "\n",
        "data.head() # display first 5 row data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ylkru32t27"
      },
      "source": [
        "# This code converts specific columns in the DataFrame 'data' to the data type 'object' for each item in the list 'catagorialList'.\n",
        "# Converting the data type to 'object' may be necessary to treat certain columns as categorical variables, enabling easier manipulation,\n",
        "# analysis, and interpretation of the data.\n",
        "catagorialList = ['sex','cp','fbs','restecg','exang','ca','thal']\n",
        "for item in catagorialList:\n",
        "    data[item] = data[item].astype('object') #casting to object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNbqP4z32t3L"
      },
      "source": [
        "# #create more data by categorical variable into indicator variables\n",
        "\n",
        "data = pd.get_dummies(data, drop_first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al2Hhn2j2t3b"
      },
      "source": [
        "data.head() # display first 5 row data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhlOEgqg2t3i"
      },
      "source": [
        "y = data['target'].values #take target value\n",
        "y = y.reshape(y.shape[0],1) #reshape y array into 1 dimension\n",
        "x = data.drop(['target'],axis=1)#drop 'target' in x data\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size"
      ],
      "metadata": {
        "id": "WkAd4I8EgjeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstration on the normalization code on the simple dataset\n",
        "\n",
        "# Create a simple dataset\n",
        "data = pd.DataFrame({'A': [10, 20, 30], 'B': [100, 200, 300], 'C': [1000, 2000, 3000]})\n",
        "print('Original dataset:')\n",
        "print(data)\n",
        "\n",
        "# Normalize data (range 0 - 1)\n",
        "minx = np.min(data)\n",
        "maxx = np.max(data)\n",
        "data_norm = (data - minx) / (maxx - minx)\n",
        "print('\\nNormalized dataset:')\n",
        "print(data_norm)"
      ],
      "metadata": {
        "id": "FOjmrSsPN1Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEGdOBJu2t3o"
      },
      "source": [
        "\n",
        "#Normalize data (range 0 - 1)\n",
        "minx = np.min(x)\n",
        "maxx = np.max(x)\n",
        "x = (x - minx) / (maxx - minx)\n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvykedw82t3u"
      },
      "source": [
        "# Split dataset into train and test. (train 70%, val 20% and test 10%)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
        "# re-create train and validation set\n",
        "x_train, x_val, y_train, y_val  = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "# train 70%, validation 20%, test 10%\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size"
      ],
      "metadata": {
        "id": "zrDehk72gtsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pwz5A_j2t30"
      },
      "source": [
        "model = Sequential() #Allow us to create model layer by layer\n",
        "model.add(Dense(64, input_dim=21, activation='softmax')) #Softmax turn number data into probabilities which sum to 1\n",
        "model.add(Dense(32, activation='softmax'))\n",
        "model.add(Dense(1, activation='sigmoid')) # produce probability value (number between 0 or 1)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0pM4z_OQfNi"
      },
      "source": [
        "#model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n",
        "model.compile(loss='mse',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999,epsilon=1e-07, amsgrad=False,name='Adam'),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unxSIBnZ2t36"
      },
      "source": [
        "# start the model training\n",
        "output = []\n",
        "early = EarlyStopping(monitor='val_acc', patience=400, mode='auto')\n",
        "checkpoint = ModelCheckpoint(model_loc+\"heart_disease_best_model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='auto', save_freq='epoch')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.01, patience=100, verbose=1, mode='auto', min_lr=0.001)\n",
        "callbacks_list = [early]\n",
        "\n",
        "output = model.fit(x_train, y_train,validation_data=(x_val,y_val), epochs=1000, batch_size=16, verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sYpy54d2t4H"
      },
      "source": [
        "plt.plot(output.history['acc'])\n",
        "plt.plot(output.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "#plt.savefig('Accuracy.png',dpi=100) #to save the image\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIrcCZ8P2t4N"
      },
      "source": [
        "plt.plot(output.history['loss'])\n",
        "plt.plot(output.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "#plt.savefig('Loss.png',dpi=100) #to save the image\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KTWzuJM12t4A"
      },
      "source": [
        "#model.load_weights(model_loc+\"heart_disease_best_model.hdf5\")\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "# print(\"\\n%s: %.2f%%\" % (model.metrics_names[0], scores[0]))\n",
        "print(\"loss:\", round(scores[0],2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNYy0CRt2t4R"
      },
      "source": [
        "#Display detailed prediction\n",
        "pred = model.predict(x_test)\n",
        "y = np.round(pred).astype(\"int16\")\n",
        "idx = 0\n",
        "ps = 0\n",
        "fl = 0\n",
        "for x in pred:\n",
        "    if y_test[idx]==y[idx]:\n",
        "        print(\"\\033[30mNo:\",idx+1,\"Actual:\",y_test[idx],\" Predicted:\",y[idx],\"Result: \\033[92mPass\")\n",
        "        ps = ps+1\n",
        "    else:\n",
        "        print(\"\\033[30mNo:\",idx+1,\"Actual:\",y_test[idx],\" Predicted:\",y[idx],\" Result: \\033[91mFail\")\n",
        "        fl = fl+1\n",
        "    idx = idx + 1\n",
        "print(\"\\033[30mRight Prediction :\",ps, \"Wrong Prediction :\",fl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size"
      ],
      "metadata": {
        "id": "Lz8Jk7kPkSWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred.size"
      ],
      "metadata": {
        "id": "C8j1-NG_lR3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHQBXNX5aYcn"
      },
      "source": [
        "y_pred = y\n",
        "y_true = y_test\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "#cm = confusion_matrix(y_true, y_pred, labels=labels.astype('int'))\n",
        "f, ax=plt.subplots(figsize=(5,5))\n",
        "sns.heatmap(cm,annot=True,linewidths=1.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\n",
        "plt.xlabel(\"y_pred\")\n",
        "plt.ylabel(\"y_true\")\n",
        "plt.show()\n",
        "print()\n",
        "print(classification_report(y_true, y_pred, labels=[0,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(y_pred)\n",
        "y_pred.size"
      ],
      "metadata": {
        "id": "I7oC8fgwfR5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOSm5uBksTLS"
      },
      "source": [
        "y_true.size"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}